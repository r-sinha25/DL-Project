{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: monai in ./.local/lib/python3.7/site-packages (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages/numpy-1.19.2-py3.7-linux-x86_64.egg (from monai) (1.19.2)\n",
      "Requirement already satisfied: torch>=1.8 in /share/pkg.7/pytorch/1.12.1/install/lib/python3.7/site-packages (from monai) (1.12.1+cu116)\n",
      "Requirement already satisfied: typing-extensions in /share/pkg.7/pytorch/1.12.1/install/lib/python3.7/site-packages (from torch>=1.8->monai) (4.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/share/pkg.7/python3/3.7.9/install/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First cell: Imports and Dataset Class\n",
    "import os\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from monai.networks.nets import ViT\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    LoadImage,\n",
    "    Resize,\n",
    "    ScaleIntensity,\n",
    "    EnsureChannelFirst,\n",
    "    RandRotate90,\n",
    "    RandFlip,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "\n",
    "class HemorrhageDataset(Dataset):\n",
    "    def __init__(self, base_dir, image_ids, transforms=None):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.image_ids = image_ids\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        folder_path = self.base_dir / f\"ID_{img_id}\"\n",
    "        \n",
    "        # Find the NIfTI file in the folder\n",
    "        nifti_files = list(folder_path.glob('*.nii.gz'))\n",
    "        if not nifti_files:\n",
    "            raise FileNotFoundError(f\"No NIfTI file found in {folder_path}\")\n",
    "        \n",
    "        # Load the NIfTI file\n",
    "        img = nib.load(str(nifti_files[0]))\n",
    "        img_data = img.get_fdata()\n",
    "        \n",
    "        # Load and parse the CSV file\n",
    "        csv_path = folder_path / 'hemorrhage_labels.csv'\n",
    "        if not csv_path.exists():\n",
    "            raise FileNotFoundError(f\"Labels file not found: {csv_path}\")\n",
    "            \n",
    "        labels_df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Extract labels (excluding the 'any' category)\n",
    "        labels = labels_df.set_index('hemorrhage_type')['label']\n",
    "        label_array = np.array([\n",
    "            labels['epidural'],\n",
    "            labels['intraparenchymal'],\n",
    "            labels['intraventricular'],\n",
    "            labels['subarachnoid'],\n",
    "            labels['subdural']\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        if self.transforms:\n",
    "            img_data = self.transforms(img_data)\n",
    "        \n",
    "        return img_data, label_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder: ID_15298ce049, Dimensions: (512, 512, 32)\n",
      "Folder: ID_a97c63329f, Dimensions: (512, 512, 37)\n",
      "Folder: ID_8f1cbd1653, Dimensions: (512, 512, 31)\n",
      "Folder: ID_1a5778261b, Dimensions: (512, 512, 34)\n",
      "Folder: ID_9f3504cb4e, Dimensions: (512, 512, 31)\n",
      "Folder: ID_b9a3bdea0c, Dimensions: (512, 512, 36)\n",
      "Folder: ID_b1ddc133e5, Dimensions: (512, 512, 28)\n",
      "Folder: ID_fcb5498cfb, Dimensions: (512, 512, 28)\n",
      "Folder: ID_014634e079, Dimensions: (512, 512, 32)\n",
      "Folder: ID_ec52c05b93, Dimensions: (512, 512, 32)\n",
      "\n",
      "Summary of dimensions found: [(512, 512, 32), (512, 512, 37), (512, 512, 31), (512, 512, 34), (512, 512, 31), (512, 512, 36), (512, 512, 28), (512, 512, 28), (512, 512, 32), (512, 512, 32)]\n"
     ]
    }
   ],
   "source": [
    "# Second cell: Dataset Analysis Function\n",
    "def analyze_dataset_dimensions(base_dir):\n",
    "    \"\"\"Analyze the dimensions of NIfTI files in the dataset\"\"\"\n",
    "    folders = list(Path(base_dir).glob('ID_*'))\n",
    "    dimensions = []\n",
    "    \n",
    "    for folder in folders[:10]:  # Analyze first 10 files as sample\n",
    "        nifti_files = list(folder.glob('*.nii.gz'))\n",
    "        if nifti_files:\n",
    "            img = nib.load(str(nifti_files[0]))\n",
    "            dimensions.append(img.shape)\n",
    "            print(f\"Folder: {folder.name}, Dimensions: {img.shape}\")\n",
    "    \n",
    "    return dimensions\n",
    "\n",
    "# Run the analysis\n",
    "base_dir = \"/projectnb/ec523kb/projects/hemorrhage-classification/stage_2_train_sorted_nifti_pruned\"  # Replace with your data directory\n",
    "dimensions = analyze_dataset_dimensions(base_dir)\n",
    "print(\"\\nSummary of dimensions found:\", dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data pipeline...\n",
      "Found 3784 total images\n",
      "Split into 300 training and 100 validation images\n",
      "Data pipeline created successfully!\n",
      "Number of training batches: 150\n",
      "Number of validation batches: 50\n",
      "\n",
      "Testing data loading...\n",
      "Successfully loaded a batch!\n",
      "Batch image shape: torch.Size([2, 1, 128, 128, 128])\n",
      "Batch labels shape: torch.Size([2, 5])\n",
      "Image data type: torch.float32\n",
      "Labels data type: torch.float32\n",
      "\n",
      "Image value range: [0.000, 0.985]\n",
      "Labels value range: [0.000, 1.000]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    EnsureChannelFirst,\n",
    "    Resize,\n",
    "    ScaleIntensity,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class HemorrhageDataset(Dataset):\n",
    "    def __init__(self, base_dir, image_ids, transforms=None):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.image_ids = image_ids\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        folder_path = self.base_dir / f\"ID_{img_id}\"\n",
    "        \n",
    "        # Find the NIfTI file in the folder\n",
    "        nifti_files = list(folder_path.glob('*.nii.gz'))\n",
    "        if not nifti_files:\n",
    "            raise FileNotFoundError(f\"No NIfTI file found in {folder_path}\")\n",
    "        \n",
    "        # Load the NIfTI file\n",
    "        img = nib.load(str(nifti_files[0]))\n",
    "        img_data = img.get_fdata()\n",
    "        \n",
    "        # Convert to tensor and add channel dimension\n",
    "        img_data = torch.from_numpy(img_data).float()\n",
    "        img_data = img_data.unsqueeze(0)  # Add channel dimension\n",
    "        \n",
    "        if self.transforms:\n",
    "            img_data = self.transforms(img_data)\n",
    "        \n",
    "        # Load and parse the CSV file\n",
    "        csv_path = folder_path / 'hemorrhage_labels.csv'\n",
    "        if not csv_path.exists():\n",
    "            raise FileNotFoundError(f\"Labels file not found: {csv_path}\")\n",
    "            \n",
    "        labels_df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Extract labels (excluding the 'any' category)\n",
    "        labels = labels_df.set_index('hemorrhage_type')['label']\n",
    "        label_array = np.array([\n",
    "            labels['epidural'],\n",
    "            labels['intraparenchymal'],\n",
    "            labels['intraventricular'],\n",
    "            labels['subarachnoid'],\n",
    "            labels['subdural']\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        label_tensor = torch.from_numpy(label_array).float()\n",
    "        \n",
    "        return img_data, label_tensor\n",
    "\n",
    "def custom_collate(batch):\n",
    "    \"\"\"Simplified collate function since all images are same size\"\"\"\n",
    "    images, labels = zip(*batch)\n",
    "    images_tensor = torch.stack(images)\n",
    "    labels_tensor = torch.stack(labels)\n",
    "    return images_tensor, labels_tensor\n",
    "\n",
    "def create_data_pipeline(base_dir, num_train=300, num_val=100):\n",
    "    \"\"\"Create data pipeline with train/val split\"\"\"\n",
    "    \n",
    "    # Get list of all image IDs\n",
    "    folders = list(Path(base_dir).glob('ID_*'))\n",
    "    image_ids = [folder.name.split('_')[1] for folder in folders]\n",
    "    \n",
    "    print(f\"Found {len(image_ids)} total images\")\n",
    "    \n",
    "    # Create train/val split\n",
    "    train_ids, val_ids = train_test_split(\n",
    "        image_ids, \n",
    "        train_size=num_train,\n",
    "        test_size=num_val,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Split into {len(train_ids)} training and {len(val_ids)} validation images\")\n",
    "    \n",
    "    # Define transforms\n",
    "    transforms = Compose([\n",
    "        ScaleIntensity(),\n",
    "        Resize((128, 128, 128), mode='trilinear')  # Add this line\n",
    "    ])\n",
    "    \n",
    "    # Create datasets with transforms\n",
    "    train_ds = HemorrhageDataset(base_dir, train_ids, transforms=transforms)\n",
    "    val_ds = HemorrhageDataset(base_dir, val_ids, transforms=transforms)\n",
    "    \n",
    "    return train_ds, val_ds\n",
    "\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "print(\"Creating data pipeline...\")\n",
    "train_ds, val_ds = create_data_pipeline(base_dir)\n",
    "\n",
    "# Create data loaders with custom collate function\n",
    "train_loader = DataLoader(\n",
    "    train_ds, \n",
    "    batch_size=2, \n",
    "    shuffle=True, \n",
    "    num_workers=2,\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    "    collate_fn=custom_collate\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds, \n",
    "    batch_size=2, \n",
    "    shuffle=False, \n",
    "    num_workers=2,\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    "    collate_fn=custom_collate\n",
    ")\n",
    "\n",
    "print(\"Data pipeline created successfully!\")\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Test the data loading\n",
    "print(\"\\nTesting data loading...\")\n",
    "try:\n",
    "    # Get the first batch\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    images, labels = sample_batch\n",
    "    \n",
    "    print(\"Successfully loaded a batch!\")\n",
    "    print(f\"Batch image shape: {images.shape}\")\n",
    "    print(f\"Batch labels shape: {labels.shape}\")\n",
    "    print(f\"Image data type: {images.dtype}\")\n",
    "    print(f\"Labels data type: {labels.dtype}\")\n",
    "    \n",
    "    # Print value ranges\n",
    "    print(f\"\\nImage value range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "    print(f\"Labels value range: [{labels.min():.3f}, {labels.max():.3f}]\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError during data loading: {str(e)}\")\n",
    "    print(\"\\nTraceback:\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in ./.local/lib/python3.7/site-packages (0.6.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/share/pkg.7/python3/3.7.9/install/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --user einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data loaders...\n",
      "Total images found: 3784\n",
      "Training images: 300\n",
      "Validation images: 100\n",
      "\n",
      "Testing batch loading...\n",
      "\n",
      "Error encountered: Caught RuntimeError in DataLoader worker process 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/share/pkg.7/pytorch/1.12.1/install/lib/SCC/../python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n",
      "    data = fetcher.fetch(index)\n",
      "  File \"/share/pkg.7/pytorch/1.12.1/install/lib/SCC/../python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"/share/pkg.7/pytorch/1.12.1/install/lib/SCC/../python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 175, in default_collate\n",
      "    return [default_collate(samples) for samples in transposed]  # Backwards compatibility.\n",
      "  File \"/share/pkg.7/pytorch/1.12.1/install/lib/SCC/../python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 175, in <listcomp>\n",
      "    return [default_collate(samples) for samples in transposed]  # Backwards compatibility.\n",
      "  File \"/share/pkg.7/pytorch/1.12.1/install/lib/SCC/../python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 141, in default_collate\n",
      "    return torch.stack(batch, 0, out=out)\n",
      "RuntimeError: stack expects each tensor to be equal size, but got [1, 512, 512, 28] at entry 0 and [1, 512, 512, 67] at entry 1\n",
      "\n",
      "\n",
      "Traceback:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-22-1b8e961f7e5d>\", line 16, in <module>\n",
      "    batch = next(iter(train_loader))\n",
      "  File \"/share/pkg.7/pytorch/1.12.1/install/lib/SCC/../python3.7/site-packages/torch/utils/data/dataloader.py\", line 681, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/share/pkg.7/pytorch/1.12.1/install/lib/SCC/../python3.7/site-packages/torch/utils/data/dataloader.py\", line 1376, in _next_data\n",
      "    return self._process_data(data)\n",
      "  File \"/share/pkg.7/pytorch/1.12.1/install/lib/SCC/../python3.7/site-packages/torch/utils/data/dataloader.py\", line 1402, in _process_data\n",
      "    data.reraise()\n",
      "  File \"/share/pkg.7/pytorch/1.12.1/install/lib/SCC/../python3.7/site-packages/torch/_utils.py\", line 461, in reraise\n",
      "    raise exception\n",
      "RuntimeError: Caught RuntimeError in DataLoader worker process 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/share/pkg.7/pytorch/1.12.1/install/lib/SCC/../python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n",
      "    data = fetcher.fetch(index)\n",
      "  File \"/share/pkg.7/pytorch/1.12.1/install/lib/SCC/../python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"/share/pkg.7/pytorch/1.12.1/install/lib/SCC/../python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 175, in default_collate\n",
      "    return [default_collate(samples) for samples in transposed]  # Backwards compatibility.\n",
      "  File \"/share/pkg.7/pytorch/1.12.1/install/lib/SCC/../python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 175, in <listcomp>\n",
      "    return [default_collate(samples) for samples in transposed]  # Backwards compatibility.\n",
      "  File \"/share/pkg.7/pytorch/1.12.1/install/lib/SCC/../python3.7/site-packages/torch/utils/data/_utils/collate.py\", line 141, in default_collate\n",
      "    return torch.stack(batch, 0, out=out)\n",
      "RuntimeError: stack expects each tensor to be equal size, but got [1, 512, 512, 28] at entry 0 and [1, 512, 512, 67] at entry 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the implementation\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = \"/projectnb/ec523kb/projects/hemorrhage-classification/stage_2_train_sorted_nifti_pruned\"\n",
    "    \n",
    "    try:\n",
    "        # Create data loaders\n",
    "        train_loader, val_loader = create_data_loaders(\n",
    "            base_dir,\n",
    "            batch_size=2,\n",
    "            train_size=300,\n",
    "            val_size=100\n",
    "        )\n",
    "        \n",
    "        # Test a batch\n",
    "        print(\"\\nTesting batch loading...\")\n",
    "        batch = next(iter(train_loader))\n",
    "        images, labels = batch\n",
    "        \n",
    "        print(f\"Batch image shape: {images.shape}\")\n",
    "        print(f\"Batch label shape: {labels.shape}\")\n",
    "        print(f\"Image value range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "        \n",
    "        print(\"\\nData loaders created successfully!\")\n",
    "        print(f\"Number of training batches: {len(train_loader)}\")\n",
    "        print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "        \n",
    "        # Print label distribution in the batch\n",
    "        print(\"\\nLabel distribution in batch:\")\n",
    "        print(\"Labels sum:\", labels.sum(dim=0))  # Sum for each class\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError encountered: {str(e)}\")\n",
    "        print(\"\\nTraceback:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Model created successfully!\n",
      "Total parameters: 19,173,893\n",
      "\n",
      "Test forward pass successful!\n",
      "Input shape: torch.Size([2, 1, 128, 128, 128])\n",
      "Output shape: torch.Size([2, 5])\n",
      "Number of hidden states: 8\n",
      "Test loss calculation: 0.6978\n",
      "\n",
      "Memory usage:\n",
      "Memory allocated: 1.19 GB\n",
      "Memory cached: 1.59 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from monai.networks.nets import ViT\n",
    "\n",
    "def create_model():\n",
    "    \"\"\"Create and configure the Vision Transformer model for hemorrhage classification\"\"\"\n",
    "    model = ViT(\n",
    "        in_channels=1,                  # Single channel CT scans\n",
    "        img_size=(128, 128, 128),       # Our resized image dimensions\n",
    "        patch_size=(16, 16, 16),        # Divide image into 8x8x8 patches\n",
    "        hidden_size=512,                # Size of hidden layers\n",
    "        mlp_dim=1024,                   # MLP dimension (2x hidden_size)\n",
    "        num_layers=8,                   # Number of transformer layers\n",
    "        num_heads=8,                    # Number of attention heads\n",
    "        pos_embed=\"conv\",               # Use conv position embedding\n",
    "        classification=True,            # Enable classification mode\n",
    "        num_classes=5,                  # Our 5 hemorrhage types\n",
    "        spatial_dims=3,                 # 3D images\n",
    "        dropout_rate=0.1                # Dropout for regularization\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def initialize_training(model, learning_rate=1e-4, weight_decay=1e-5):\n",
    "    \"\"\"Initialize optimizer and loss function\"\"\"\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    criterion = nn.BCEWithLogitsLoss()  # Binary cross-entropy for multi-label\n",
    "    return optimizer, criterion\n",
    "\n",
    "# Test the model\n",
    "if __name__ == \"__main__\":\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    try:\n",
    "        # Create model\n",
    "        model = create_model()\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Initialize training components\n",
    "        optimizer, criterion = initialize_training(model)\n",
    "        \n",
    "        # Print model summary\n",
    "        print(\"\\nModel created successfully!\")\n",
    "        print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "        \n",
    "        # Test with a sample batch from our dataloader\n",
    "        batch = next(iter(train_loader))\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, hidden_states = model(images)\n",
    "        \n",
    "        print(\"\\nTest forward pass successful!\")\n",
    "        print(f\"Input shape: {images.shape}\")\n",
    "        print(f\"Output shape: {outputs.shape}\")\n",
    "        print(f\"Number of hidden states: {len(hidden_states)}\")\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        print(f\"Test loss calculation: {loss.item():.4f}\")\n",
    "        \n",
    "        print(\"\\nMemory usage:\")\n",
    "        print(f\"Memory allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "        print(f\"Memory cached: {torch.cuda.memory_reserved()/1e9:.2f} GB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError encountered: {str(e)}\")\n",
    "        print(\"\\nTraceback:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|          | 0/150 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 150/150 [00:27<00:00,  5.54it/s, Loss=0.3960]\n",
      "Validation:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "roc_auc: 0.4831\n",
      "avg_precision: 0.0888\n",
      "roc_auc_per_class: [0.33501684 0.47422138 0.50495846 0.50629572 0.5952067 ]\n",
      "avg_precision_per_class: [0.00956839 0.10950379 0.0498298  0.11234712 0.16273345]\n",
      "loss: 0.3960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  76%|███████▌  | 38/50 [00:07<00:02,  5.21it/s, Loss=0.3896]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"/usr4/ec523/ibhattac/.local/lib/python3.7/site-packages/monai/transforms/transform.py\", line 102, in apply_transform\n    return _apply_transform(transform, data, unpack_items)\n  File \"/usr4/ec523/ibhattac/.local/lib/python3.7/site-packages/monai/transforms/transform.py\", line 66, in _apply_transform\n    return transform(parameters)\n  File \"/usr4/ec523/ibhattac/.local/lib/python3.7/site-packages/monai/transforms/croppad/array.py\", line 1322, in __call__\n    ret = self.padder(self.cropper(img), mode=mode, **pad_kwargs)\n  File \"/usr4/ec523/ibhattac/.local/lib/python3.7/site-packages/monai/transforms/croppad/array.py\", line 543, in __call__\n    return super().__call__(img=img, slices=self.compute_slices(img.shape[1:]))\n  File \"/usr4/ec523/ibhattac/.local/lib/python3.7/site-packages/monai/transforms/croppad/array.py\", line 533, in compute_slices\n    roi_size = fall_back_tuple(self.roi_size, spatial_size)\n  File \"/usr4/ec523/ibhattac/.local/lib/python3.7/site-packages/monai/utils/misc.py\", line 202, in fall_back_tuple\n    user = ensure_tuple_rep(user_provided, ndim)\n  File \"/usr4/ec523/ibhattac/.local/lib/python3.7/site-packages/monai/utils/misc.py\", line 159, in ensure_tuple_rep\n    raise ValueError(f\"Sequence must have length {dim}, got {len(tup)}.\")\nValueError: Sequence must have length 4, got 3.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/share/pkg.7/pytorch/1.12.1/install/lib/SCC/../python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/share/pkg.7/pytorch/1.12.1/install/lib/SCC/../python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/share/pkg.7/pytorch/1.12.1/install/lib/SCC/../python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-16-d72bec1c6f2a>\", line 44, in __getitem__\n    img_data = self.transforms(img_data)\n  File \"/usr4/ec523/ibhattac/.local/lib/python3.7/site-packages/monai/transforms/compose.py\", line 174, in __call__\n    input_ = apply_transform(_transform, input_, self.map_items, self.unpack_items, self.log_stats)\n  File \"/usr4/ec523/ibhattac/.local/lib/python3.7/site-packages/monai/transforms/transform.py\", line 129, in apply_transform\n    raise RuntimeError(f\"applying transform {transform}\") from e\nRuntimeError: applying transform <monai.transforms.croppad.array.ResizeWithPadOrCrop object at 0x14c6ad922110>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-8acb27a856c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m     )\n",
      "\u001b[0;32m<ipython-input-20-8acb27a856c9>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, checkpoint_dir)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m# Validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mval_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nValidation Metrics:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-8acb27a856c9>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, val_loader, criterion, device)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1172\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.12.1/install/lib/SCC/../python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.12.1/install/lib/SCC/../python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.12.1/install/lib/SCC/../python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.12.1/install/lib/SCC/../python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"/usr4/ec523/ibhattac/.local/lib/python3.7/site-packages/monai/transforms/transform.py\", line 102, in apply_transform\n    return _apply_transform(transform, data, unpack_items)\n  File \"/usr4/ec523/ibhattac/.local/lib/python3.7/site-packages/monai/transforms/transform.py\", line 66, in _apply_transform\n    return transform(parameters)\n  File \"/usr4/ec523/ibhattac/.local/lib/python3.7/site-packages/monai/transforms/croppad/array.py\", line 1322, in __call__\n    ret = self.padder(self.cropper(img), mode=mode, **pad_kwargs)\n  File \"/usr4/ec523/ibhattac/.local/lib/python3.7/site-packages/monai/transforms/croppad/array.py\", line 543, in __call__\n    return super().__call__(img=img, slices=self.compute_slices(img.shape[1:]))\n  File \"/usr4/ec523/ibhattac/.local/lib/python3.7/site-packages/monai/transforms/croppad/array.py\", line 533, in compute_slices\n    roi_size = fall_back_tuple(self.roi_size, spatial_size)\n  File \"/usr4/ec523/ibhattac/.local/lib/python3.7/site-packages/monai/utils/misc.py\", line 202, in fall_back_tuple\n    user = ensure_tuple_rep(user_provided, ndim)\n  File \"/usr4/ec523/ibhattac/.local/lib/python3.7/site-packages/monai/utils/misc.py\", line 159, in ensure_tuple_rep\n    raise ValueError(f\"Sequence must have length {dim}, got {len(tup)}.\")\nValueError: Sequence must have length 4, got 3.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/share/pkg.7/pytorch/1.12.1/install/lib/SCC/../python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/share/pkg.7/pytorch/1.12.1/install/lib/SCC/../python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/share/pkg.7/pytorch/1.12.1/install/lib/SCC/../python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-16-d72bec1c6f2a>\", line 44, in __getitem__\n    img_data = self.transforms(img_data)\n  File \"/usr4/ec523/ibhattac/.local/lib/python3.7/site-packages/monai/transforms/compose.py\", line 174, in __call__\n    input_ = apply_transform(_transform, input_, self.map_items, self.unpack_items, self.log_stats)\n  File \"/usr4/ec523/ibhattac/.local/lib/python3.7/site-packages/monai/transforms/transform.py\", line 129, in apply_transform\n    raise RuntimeError(f\"applying transform {transform}\") from e\nRuntimeError: applying transform <monai.transforms.croppad.array.ResizeWithPadOrCrop object at 0x14c6ad922110>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import time\n",
    "from pathlib import Path\n",
    "import copy\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def calculate_metrics(labels, outputs):\n",
    "    \"\"\"Calculate metrics for multi-label classification\"\"\"\n",
    "    # Convert outputs to probabilities\n",
    "    probs = torch.sigmoid(outputs)\n",
    "    \n",
    "    # Move tensors to CPU and convert to numpy\n",
    "    labels_np = labels.cpu().numpy()\n",
    "    probs_np = probs.detach().cpu().numpy()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    try:\n",
    "        # ROC AUC for each class\n",
    "        roc_auc = roc_auc_score(labels_np, probs_np, average=None)\n",
    "        mean_roc_auc = np.mean(roc_auc)\n",
    "        \n",
    "        # Average Precision for each class\n",
    "        avg_precision = average_precision_score(labels_np, probs_np, average=None)\n",
    "        mean_avg_precision = np.mean(avg_precision)\n",
    "        \n",
    "        return {\n",
    "            'roc_auc': mean_roc_auc,\n",
    "            'avg_precision': mean_avg_precision,\n",
    "            'roc_auc_per_class': roc_auc,\n",
    "            'avg_precision_per_class': avg_precision\n",
    "        }\n",
    "    except ValueError:\n",
    "        return {\n",
    "            'roc_auc': 0.0,\n",
    "            'avg_precision': 0.0,\n",
    "            'roc_auc_per_class': np.zeros(5),\n",
    "            'avg_precision_per_class': np.zeros(5)\n",
    "        }\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, scheduler, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for images, labels in pbar:\n",
    "        # Move data to device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update metrics\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        all_labels.append(labels)\n",
    "        all_outputs.append(outputs)\n",
    "        \n",
    "        pbar.set_postfix({'Loss': f'{losses.avg:.4f}'})\n",
    "    \n",
    "    # Calculate epoch metrics\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    all_outputs = torch.cat(all_outputs)\n",
    "    metrics = calculate_metrics(all_labels, all_outputs)\n",
    "    metrics['loss'] = losses.avg\n",
    "    \n",
    "    # Step the scheduler\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc='Validation')\n",
    "        for images, labels in pbar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs, _ = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Update metrics\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            all_labels.append(labels)\n",
    "            all_outputs.append(outputs)\n",
    "            \n",
    "            pbar.set_postfix({'Loss': f'{losses.avg:.4f}'})\n",
    "    \n",
    "    # Calculate metrics\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    all_outputs = torch.cat(all_outputs)\n",
    "    metrics = calculate_metrics(all_labels, all_outputs)\n",
    "    metrics['loss'] = losses.avg\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                num_epochs, device, checkpoint_dir='checkpoints'):\n",
    "    \"\"\"Complete training pipeline\"\"\"\n",
    "    # Create checkpoint directory\n",
    "    checkpoint_dir = Path(checkpoint_dir)\n",
    "    checkpoint_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Initialize learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=1e-3,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.3,  # Spend 30% of training warming up\n",
    "        div_factor=25.0,  # Initial lr = max_lr/25\n",
    "        final_div_factor=10000.0  # Final lr = max_lr/10000\n",
    "    )\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_val_auc = 0.0\n",
    "    best_model = None\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 20)\n",
    "        \n",
    "        # Train\n",
    "        train_metrics = train_epoch(model, train_loader, criterion, optimizer, scheduler, device)\n",
    "        print('\\nTraining Metrics:')\n",
    "        for k, v in train_metrics.items():\n",
    "            if isinstance(v, np.ndarray):\n",
    "                print(f'{k}: {v}')\n",
    "            else:\n",
    "                print(f'{k}: {v:.4f}')\n",
    "        \n",
    "        # Validate\n",
    "        val_metrics = validate(model, val_loader, criterion, device)\n",
    "        print('\\nValidation Metrics:')\n",
    "        for k, v in val_metrics.items():\n",
    "            if isinstance(v, np.ndarray):\n",
    "                print(f'{k}: {v}')\n",
    "            else:\n",
    "                print(f'{k}: {v:.4f}')\n",
    "        \n",
    "        # Save best model by validation loss\n",
    "        if val_metrics['loss'] < best_val_loss:\n",
    "            best_val_loss = val_metrics['loss']\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'metrics': val_metrics,\n",
    "            }, checkpoint_dir / 'best_loss_model.pth')\n",
    "        \n",
    "        # Save best model by ROC AUC\n",
    "        if val_metrics['roc_auc'] > best_val_auc:\n",
    "            best_val_auc = val_metrics['roc_auc']\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'metrics': val_metrics,\n",
    "            }, checkpoint_dir / 'best_auc_model.pth')\n",
    "        \n",
    "        # Save periodic checkpoint\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'metrics': val_metrics,\n",
    "            }, checkpoint_dir / f'epoch_{epoch+1}_checkpoint.pth')\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "# Training configuration and execution\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    num_epochs = 50\n",
    "    checkpoint_dir = 'hemorrhage_checkpoints'\n",
    "    \n",
    "    # Train the model\n",
    "    best_model_state = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=num_epochs,\n",
    "        device=device,\n",
    "        checkpoint_dir=checkpoint_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
