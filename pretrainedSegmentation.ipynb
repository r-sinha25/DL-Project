{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fslpy==2.7.0 in ./.local/lib/python3.7/site-packages (2.7.0)\n",
      "Requirement already satisfied: numpy==1.* in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from fslpy==2.7.0) (1.18.1)\n",
      "Requirement already satisfied: h5py>=2.9 in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from fslpy==2.7.0) (2.10.0)\n",
      "Requirement already satisfied: six==1.* in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from fslpy==2.7.0) (1.14.0)\n",
      "Requirement already satisfied: scipy>=0.18 in /share/pkg.7/tensorflow/2.1.0/install/lib/python3.7-gpu/site-packages (from fslpy==2.7.0) (1.4.1)\n",
      "Requirement already satisfied: nibabel>=2.3.1 in /share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages (from fslpy==2.7.0) (3.2.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages (from nibabel>=2.3.1->fslpy==2.7.0) (20.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages (from packaging>=14.3->nibabel>=2.3.1->fslpy==2.7.0) (2.4.7)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/share/pkg.7/python3/3.7.9/install/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install fslpy==2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.9 (default, Oct 26 2020, 11:27:26) \n",
      "[GCC 4.8.5 20150623 (Red Hat 4.8.5-39)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: antspyx==0.1.7 from https://github.com/ANTsX/ANTsPy/releases/download/v0.1.8/antspyx-0.1.7-cp37-cp37m-linux_x86_64.whl in ./.local/lib/python3.7/site-packages (0.1.7)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/share/pkg.7/python3/3.7.9/install/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --user https://github.com/ANTsX/ANTsPy/releases/download/v0.1.8/antspyx-0.1.7-cp37-cp37m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the root directory path: /projectnb/ec523kb/projects/hemorrhage-classification/stage_2_train_sorted_nifti_pruned\n",
      "Enter number of images for subset (default 10): 10\n",
      "Enter desired ratio of hemorrhage cases (0-1, default 0.5): 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 01:27:06,778 - SubsetCreator - INFO - Analyzing images for hemorrhage status...\n",
      "Checking images:   0%|          | 0/4492 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating balanced data subset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking images: 100%|██████████| 4492/4492 [01:09<00:00, 64.99it/s]\n",
      "2024-12-10 01:28:15,970 - SubsetCreator - INFO - Found 1829 hemorrhage cases and 2663 non-hemorrhage cases\n",
      "2024-12-10 01:28:15,971 - SubsetCreator - INFO - Creating subset with 10 images (5 hemorrhage, 5 non-hemorrhage)\n",
      "2024-12-10 01:28:16,434 - SubsetCreator - INFO - \n",
      "Subset created in: /projectnb/ec523kb/projects/hemorrhage-classification/stage_2_train_sorted_nifti_pruned/subset_test\n",
      "2024-12-10 01:28:16,434 - SubsetCreator - INFO - Total images in subset: 10\n",
      "2024-12-10 01:28:16,435 - SubsetCreator - INFO - Hemorrhage cases: 5\n",
      "2024-12-10 01:28:16,435 - SubsetCreator - INFO - Non-hemorrhage cases: 5\n",
      "Processing subset:   0%|          | 0/10 [00:00<?, ?it/s]2024-12-10 01:28:16,449 - DeepBleed - INFO - Running: python3 predict.py --verbose --indir /projectnb/ec523kb/projects/hemorrhage-classification/stage_2_train_sorted_nifti_pruned/subset_test/deepbleed_input --outdir /projectnb/ec523kb/projects/hemorrhage-classification/stage_2_train_sorted_nifti_pruned/subset_test/deepbleed_output --weights weights\n",
      "2024-12-10 01:28:16,449 - DeepBleed - ERROR - Error processing ID_cf7548e6b2_Eq_1.nii.gz: name 'subprocess' is not defined\n",
      "2024-12-10 01:28:16,464 - DeepBleed - INFO - Running: python3 predict.py --verbose --indir /projectnb/ec523kb/projects/hemorrhage-classification/stage_2_train_sorted_nifti_pruned/subset_test/deepbleed_input --outdir /projectnb/ec523kb/projects/hemorrhage-classification/stage_2_train_sorted_nifti_pruned/subset_test/deepbleed_output --weights weights\n",
      "2024-12-10 01:28:16,464 - DeepBleed - ERROR - Error processing ID_efde187adf_Eq_1.nii.gz: name 'subprocess' is not defined\n",
      "2024-12-10 01:28:16,478 - DeepBleed - INFO - Running: python3 predict.py --verbose --indir /projectnb/ec523kb/projects/hemorrhage-classification/stage_2_train_sorted_nifti_pruned/subset_test/deepbleed_input --outdir /projectnb/ec523kb/projects/hemorrhage-classification/stage_2_train_sorted_nifti_pruned/subset_test/deepbleed_output --weights weights\n",
      "2024-12-10 01:28:16,478 - DeepBleed - ERROR - Error processing ID_874903e1d4.nii.gz: name 'subprocess' is not defined\n",
      "2024-12-10 01:28:16,494 - DeepBleed - INFO - Running: python3 predict.py --verbose --indir /projectnb/ec523kb/projects/hemorrhage-classification/stage_2_train_sorted_nifti_pruned/subset_test/deepbleed_input --outdir /projectnb/ec523kb/projects/hemorrhage-classification/stage_2_train_sorted_nifti_pruned/subset_test/deepbleed_output --weights weights\n",
      "2024-12-10 01:28:16,495 - DeepBleed - ERROR - Error processing ID_7926b620af.nii.gz: name 'subprocess' is not defined\n",
      "2024-12-10 01:28:16,511 - DeepBleed - INFO - Running: python3 predict.py --verbose --indir /projectnb/ec523kb/projects/hemorrhage-classification/stage_2_train_sorted_nifti_pruned/subset_test/deepbleed_input --outdir /projectnb/ec523kb/projects/hemorrhage-classification/stage_2_train_sorted_nifti_pruned/subset_test/deepbleed_output --weights weights\n",
      "2024-12-10 01:28:16,511 - DeepBleed - ERROR - Error processing ID_a07b145374.nii.gz: name 'subprocess' is not defined\n",
      "2024-12-10 01:28:16,526 - DeepBleed - INFO - Running: python3 predict.py --verbose --indir /projectnb/ec523kb/projects/hemorrhage-classification/stage_2_train_sorted_nifti_pruned/subset_test/deepbleed_input --outdir /projectnb/ec523kb/projects/hemorrhage-classification/stage_2_train_sorted_nifti_pruned/subset_test/deepbleed_output --weights weights\n",
      "2024-12-10 01:28:16,526 - DeepBleed - ERROR - Error processing ID_c3c50e3001_Tilt_Eq_1.nii.gz: name 'subprocess' is not defined\n",
      "2024-12-10 01:28:16,539 - DeepBleed - INFO - Running: python3 predict.py --verbose --indir /projectnb/ec523kb/projects/hemorrhage-classification/stage_2_train_sorted_nifti_pruned/subset_test/deepbleed_input --outdir /projectnb/ec523kb/projects/hemorrhage-classification/stage_2_train_sorted_nifti_pruned/subset_test/deepbleed_output --weights weights\n",
      "2024-12-10 01:28:16,539 - DeepBleed - ERROR - Error processing ID_a59b329e9e_Eq_1.nii.gz: name 'subprocess' is not defined\n",
      "Processing subset:  70%|███████   | 7/10 [00:00<00:00, 68.27it/s]2024-12-10 01:28:16,556 - DeepBleed - INFO - Running: python3 predict.py --verbose --indir /projectnb/ec523kb/projects/hemorrhage-classification/stage_2_train_sorted_nifti_pruned/subset_test/deepbleed_input --outdir /projectnb/ec523kb/projects/hemorrhage-classification/stage_2_train_sorted_nifti_pruned/subset_test/deepbleed_output --weights weights\n",
      "2024-12-10 01:28:16,556 - DeepBleed - ERROR - Error processing ID_d48b3c4453.nii.gz: name 'subprocess' is not defined\n",
      "2024-12-10 01:28:16,571 - DeepBleed - INFO - Running: python3 predict.py --verbose --indir /projectnb/ec523kb/projects/hemorrhage-classification/stage_2_train_sorted_nifti_pruned/subset_test/deepbleed_input --outdir /projectnb/ec523kb/projects/hemorrhage-classification/stage_2_train_sorted_nifti_pruned/subset_test/deepbleed_output --weights weights\n",
      "2024-12-10 01:28:16,571 - DeepBleed - ERROR - Error processing ID_77969addb0.nii.gz: name 'subprocess' is not defined\n",
      "2024-12-10 01:28:16,589 - DeepBleed - INFO - Running: python3 predict.py --verbose --indir /projectnb/ec523kb/projects/hemorrhage-classification/stage_2_train_sorted_nifti_pruned/subset_test/deepbleed_input --outdir /projectnb/ec523kb/projects/hemorrhage-classification/stage_2_train_sorted_nifti_pruned/subset_test/deepbleed_output --weights weights\n",
      "2024-12-10 01:28:16,590 - DeepBleed - ERROR - Error processing ID_dfe0a53852.nii.gz: name 'subprocess' is not defined\n",
      "Processing subset: 100%|██████████| 10/10 [00:00<00:00, 65.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running DeepBleed segmentation on subset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "def setup_logging(root_directory):\n",
    "    \"\"\"Setup logging configuration\"\"\"\n",
    "    log_dir = Path(root_directory) / 'logs'\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_dir / 'subset_creation.log'),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger('SubsetCreator')\n",
    "\n",
    "def read_hemorrhage_labels(csv_path):\n",
    "    \"\"\"Read hemorrhage labels from CSV file\"\"\"\n",
    "    labels = {}\n",
    "    try:\n",
    "        with open(csv_path, 'r') as f:\n",
    "            csv_reader = csv.reader(f)\n",
    "            next(csv_reader)  # Skip header row\n",
    "            for row in csv_reader:\n",
    "                if len(row) == 2:\n",
    "                    h_type, label = row\n",
    "                    labels[h_type] = int(label)\n",
    "        return labels\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {csv_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_image_hemorrhage_status(image_id, root_path):\n",
    "    \"\"\"Check if image has any hemorrhage type\"\"\"\n",
    "    # Find corresponding hemorrhage_labels.csv\n",
    "    for subdir in root_path.glob('ID_*'):\n",
    "        if subdir.is_dir() and image_id in subdir.name:\n",
    "            csv_path = subdir / 'hemorrhage_labels.csv'\n",
    "            if csv_path.exists():\n",
    "                labels = read_hemorrhage_labels(csv_path)\n",
    "                if labels:\n",
    "                    # Check for any hemorrhage type except 'any'\n",
    "                    return any(labels[h_type] == 1 for h_type in labels if h_type != 'any')\n",
    "    return False\n",
    "\n",
    "def create_balanced_subset(root_directory, subset_size=10, hemorrhage_ratio=0.5, seed=42):\n",
    "    \"\"\"\n",
    "    Create a balanced subset of the data\n",
    "    \n",
    "    Args:\n",
    "        root_directory (str): Root directory containing original data\n",
    "        subset_size (int): Number of images to include in subset\n",
    "        hemorrhage_ratio (float): Desired ratio of hemorrhage cases (0-1)\n",
    "        seed (int): Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    logger = setup_logging(root_directory)\n",
    "    root_path = Path(root_directory)\n",
    "    \n",
    "    # Setup directories\n",
    "    train_dir = root_path / 'trainingImages'\n",
    "    subset_dir = root_path / 'subset_test'\n",
    "    subset_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Get all images and their hemorrhage status\n",
    "    logger.info(\"Analyzing images for hemorrhage status...\")\n",
    "    image_status = {}\n",
    "    for img_path in tqdm(list(train_dir.glob('*.nii*')), desc=\"Checking images\"):\n",
    "        image_id = img_path.stem.split('.')[0]  # Remove extension\n",
    "        if image_id.endswith('_Eq_1'):\n",
    "            image_id = image_id[:-5]\n",
    "        has_hemorrhage = get_image_hemorrhage_status(image_id, root_path)\n",
    "        image_status[img_path] = has_hemorrhage\n",
    "    \n",
    "    # Split into hemorrhage and non-hemorrhage cases\n",
    "    hemorrhage_cases = [path for path, has_hem in image_status.items() if has_hem]\n",
    "    non_hemorrhage_cases = [path for path, has_hem in image_status.items() if not has_hem]\n",
    "    \n",
    "    logger.info(f\"Found {len(hemorrhage_cases)} hemorrhage cases and {len(non_hemorrhage_cases)} non-hemorrhage cases\")\n",
    "    \n",
    "    # Calculate numbers for balanced subset\n",
    "    num_hemorrhage = int(subset_size * hemorrhage_ratio)\n",
    "    num_non_hemorrhage = subset_size - num_hemorrhage\n",
    "    \n",
    "    # Randomly select cases\n",
    "    random.seed(seed)\n",
    "    selected_hemorrhage = random.sample(hemorrhage_cases, min(num_hemorrhage, len(hemorrhage_cases)))\n",
    "    selected_non_hemorrhage = random.sample(non_hemorrhage_cases, min(num_non_hemorrhage, len(non_hemorrhage_cases)))\n",
    "    \n",
    "    # Combine selections\n",
    "    selected_images = selected_hemorrhage + selected_non_hemorrhage\n",
    "    \n",
    "    # Copy selected images to subset directory\n",
    "    logger.info(f\"Creating subset with {len(selected_images)} images \"\n",
    "               f\"({len(selected_hemorrhage)} hemorrhage, {len(selected_non_hemorrhage)} non-hemorrhage)\")\n",
    "    \n",
    "    # Save selection details\n",
    "    with open(subset_dir / 'subset_info.txt', 'w') as f:\n",
    "        f.write(\"Subset Contents:\\n\\n\")\n",
    "        f.write(\"Hemorrhage Cases:\\n\")\n",
    "        for img_path in selected_hemorrhage:\n",
    "            f.write(f\"{img_path.name}\\n\")\n",
    "            shutil.copy2(img_path, subset_dir / img_path.name)\n",
    "            \n",
    "        f.write(\"\\nNon-hemorrhage Cases:\\n\")\n",
    "        for img_path in selected_non_hemorrhage:\n",
    "            f.write(f\"{img_path.name}\\n\")\n",
    "            shutil.copy2(img_path, subset_dir / img_path.name)\n",
    "    \n",
    "    logger.info(f\"\\nSubset created in: {subset_dir}\")\n",
    "    logger.info(f\"Total images in subset: {len(selected_images)}\")\n",
    "    logger.info(f\"Hemorrhage cases: {len(selected_hemorrhage)}\")\n",
    "    logger.info(f\"Non-hemorrhage cases: {len(selected_non_hemorrhage)}\")\n",
    "    \n",
    "    return subset_dir\n",
    "\n",
    "def run_segmentation_on_subset(subset_dir):\n",
    "    \"\"\"Run DeepBleed segmentation on the subset\"\"\"\n",
    "    logger = logging.getLogger('DeepBleed')\n",
    "    \n",
    "    # Setup directories for DeepBleed\n",
    "    input_dir = subset_dir / 'deepbleed_input'\n",
    "    output_dir = subset_dir / 'deepbleed_output'\n",
    "    \n",
    "    input_dir.mkdir(exist_ok=True)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Process subset images\n",
    "    for image_file in tqdm(list(subset_dir.glob('*.nii*')), desc=\"Processing subset\"):\n",
    "        try:\n",
    "            if image_file.suffix == '.txt':\n",
    "                continue\n",
    "                \n",
    "            # Clear and recreate input directory\n",
    "            if input_dir.exists():\n",
    "                shutil.rmtree(str(input_dir))\n",
    "            input_dir.mkdir()\n",
    "            \n",
    "            # Copy image to input directory\n",
    "            input_path = input_dir / image_file.name\n",
    "            shutil.copy2(image_file, input_path)\n",
    "            \n",
    "            # Run DeepBleed prediction\n",
    "            cmd = f\"python3 predict.py --verbose --indir {input_dir} --outdir {output_dir} --weights weights\"\n",
    "            logger.info(f\"Running: {cmd}\")\n",
    "            \n",
    "            result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "            \n",
    "            if result.returncode != 0:\n",
    "                logger.error(f\"Error processing {image_file.name}:\")\n",
    "                logger.error(result.stderr)\n",
    "            else:\n",
    "                logger.info(f\"Successfully processed {image_file.name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {image_file.name}: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        root_dir = input(\"Enter the root directory path: \").strip()\n",
    "        subset_size = int(input(\"Enter number of images for subset (default 10): \").strip() or \"10\")\n",
    "        hemorrhage_ratio = float(input(\"Enter desired ratio of hemorrhage cases (0-1, default 0.5): \").strip() or \"0.5\")\n",
    "        \n",
    "        # Create balanced subset\n",
    "        print(\"\\nCreating balanced data subset...\")\n",
    "        subset_dir = create_balanced_subset(root_dir, subset_size, hemorrhage_ratio)\n",
    "        \n",
    "        # Run segmentation\n",
    "        print(\"\\nRunning DeepBleed segmentation on subset...\")\n",
    "        run_segmentation_on_subset(subset_dir)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES: 1\n",
      "GPUs available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA and GPU availability\n",
    "import os\n",
    "import tensorflow as tf\n",
    "print(\"CUDA_VISIBLE_DEVICES:\", os.environ.get('CUDA_VISIBLE_DEVICES'))\n",
    "print(\"GPUs available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths set up:\n",
      "Input directory: /projectnb/ec523kb/projects/hemorrhage-classification/stage_2_train_sorted_nifti_pruned/subset_test/deepbleed_input\n",
      "Output directory: /projectnb/ec523kb/projects/hemorrhage-classification/stage_2_train_sorted_nifti_pruned/subset_test/deepbleed_output\n",
      "Weights directory: /projectnb/ec523kb/projects/hemorrhage-classification/stage_2_train_sorted_nifti_pruned/deepbleed/weights\n",
      "\n",
      "Current working directory: /projectnb/ec523kb/projects/hemorrhage-classification/stage_2_train_sorted_nifti_pruned/deepbleed\n",
      "\n",
      "Running prediction...\n",
      "Command: python3 predict.py --indir /projectnb/ec523kb/projects/hemorrhage-classification/stage_2_train_sorted_nifti_pruned/subset_test/deepbleed_input --outdir /projectnb/ec523kb/projects/hemorrhage-classification/stage_2_train_sorted_nifti_pruned/subset_test/deepbleed_output --weights /projectnb/ec523kb/projects/hemorrhage-classification/stage_2_train_sorted_nifti_pruned/deepbleed/weights/weights\n",
      "\n",
      "Output: ANTsImage (LAI)\n",
      "\t Pixel Type : float (float32)\n",
      "\t Components : 1\n",
      "\t Dimensions : (512, 512, 32)\n",
      "\t Spacing    : (0.4883, 0.4883, 5.0)\n",
      "\t Origin     : (125.0, -109.0116, -11.25)\n",
      "\t Direction  : [-1.  0.  0.  0.  1.  0.  0.  0.  1.]\n",
      "\n",
      "ANTsImage (LAI)\n",
      "\t Pixel Type : float (float32)\n",
      "\t Components : 1\n",
      "\t Dimensions : (512, 512, 36)\n",
      "\t Spacing    : (0.4883, 0.4883, 5.0)\n",
      "\t Origin     : (125.0, -245.5117, 49.9)\n",
      "\t Direction  : [-1.  0.  0.  0.  1.  0.  0.  0.  1.]\n",
      "\n",
      "ANTsImage (LAI)\n",
      "\t Pixel Type : float (float32)\n",
      "\t Components : 1\n",
      "\t Dimensions : (512, 512, 28)\n",
      "\t Spacing    : (0.4883, 0.4883, 5.7168)\n",
      "\t Origin     : (125.0, -108.9003, -145.8645)\n",
      "\t Direction  : [-1.      0.      0.      0.      0.8746 -0.4848  0.      0.4848  0.8746]\n",
      "\n",
      "ANTsImage (LAI)\n",
      "\t Pixel Type : float (float32)\n",
      "\t Components : 1\n",
      "\t Dimensions : (512, 512, 35)\n",
      "\t Spacing    : (0.4883, 0.4883, 5.0)\n",
      "\t Origin     : (95.648, -124.5116, -96.959)\n",
      "\t Direction  : [-1.  0.  0.  0.  1.  0.  0.  0.  1.]\n",
      "\n",
      "ANTsImage (LAI)\n",
      "\t Pixel Type : float (float32)\n",
      "\t Components : 1\n",
      "\t Dimensions : (512, 512, 31)\n",
      "\t Spacing    : (0.4883, 0.4883, 5.392)\n",
      "\t Origin     : (125.0, -147.0451, -41.4102)\n",
      "\t Direction  : [-1.      0.      0.      0.      0.9272 -0.3746  0.      0.3746  0.9272]\n",
      "\n",
      "ANTsImage (LAI)\n",
      "\t Pixel Type : float (float32)\n",
      "\t Components : 1\n",
      "\t Dimensions : (512, 632, 29)\n",
      "\t Spacing    : (0.4883, 0.4883, 4.9209)\n",
      "\t Origin     : (125.0, -270.7365, 26.9472)\n",
      "\t Direction  : [-1.000e+00 -1.000e-04 -4.000e-04  1.000e-04  9.272e-01 -3.746e-01\n",
      " -4.000e-04  3.746e-01  9.272e-01]\n",
      "\n",
      "ANTsImage (LAI)\n",
      "\t Pixel Type : float (float32)\n",
      "\t Components : 1\n",
      "\t Dimensions : (512, 512, 27)\n",
      "\t Spacing    : (0.4883, 0.4883, 5.2427)\n",
      "\t Origin     : (125.0, -282.054, 137.8246)\n",
      "\t Direction  : [-1.      0.      0.      0.      0.9511 -0.309   0.      0.309   0.9511]\n",
      "\n",
      "ANTsImage (LAI)\n",
      "\t Pixel Type : float (float32)\n",
      "\t Components : 1\n",
      "\t Dimensions : (512, 512, 36)\n",
      "\t Spacing    : (0.4883, 0.4883, 5.3927)\n",
      "\t Origin     : (125.0, -125.3451, -117.6803)\n",
      "\t Direction  : [-1.      0.      0.      0.      0.9272 -0.3746  0.      0.3746  0.9272]\n",
      "\n",
      "ANTsImage (LAI)\n",
      "\t Pixel Type : float (float32)\n",
      "\t Components : 1\n",
      "\t Dimensions : (512, 512, 40)\n",
      "\t Spacing    : (0.4883, 0.4883, 2.758)\n",
      "\t Origin     : (125.0, -115.4463, -68.5831)\n",
      "\t Direction  : [-1.      0.      0.      0.      0.9063 -0.4226  0.      0.4226  0.9063]\n",
      "\n",
      "ANTsImage (LAI)\n",
      "\t Pixel Type : float (float32)\n",
      "\t Components : 1\n",
      "\t Dimensions : (512, 512, 29)\n",
      "\t Spacing    : (0.4648, 0.4648, 5.3074)\n",
      "\t Origin     : (119.0, -262.0689, -2.8187)\n",
      "\t Direction  : [-1.000e+00 -1.000e-04 -4.000e-04  1.000e-04  9.272e-01 -3.746e-01\n",
      " -4.000e-04  3.746e-01  9.272e-01]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Set paths\n",
    "subset_dir = Path(\"/projectnb/ec523kb/projects/hemorrhage-classification/stage_2_train_sorted_nifti_pruned/subset_test\")\n",
    "deepbleed_dir = subset_dir.parent / 'deepbleed'\n",
    "input_dir = subset_dir / 'deepbleed_input'\n",
    "output_dir = subset_dir / 'deepbleed_output'\n",
    "weights_dir = deepbleed_dir / 'weights'\n",
    "\n",
    "print(\"Paths set up:\")\n",
    "print(f\"Input directory: {input_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Weights directory: {weights_dir}\")\n",
    "\n",
    "# Create directories\n",
    "input_dir.mkdir(exist_ok=True)\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "weights_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Clone DeepBleed if not exists\n",
    "if not deepbleed_dir.exists():\n",
    "    print(\"\\nCloning DeepBleed...\")\n",
    "    subprocess.run(\n",
    "        f\"git clone https://github.com/msharrock/deepbleed.git {deepbleed_dir}\",\n",
    "        shell=True, check=True\n",
    "    )\n",
    "\n",
    "# Download and set up weights\n",
    "if not (weights_dir / 'weights.index').exists():\n",
    "    print(\"\\nDownloading weights...\")\n",
    "    os.chdir(str(weights_dir))\n",
    "    subprocess.run(\"wget -O weights.zip https://www.dropbox.com/s/v2ptd9mfpo13gcb/mistie_2-20200122T175000Z-001.zip?dl=1\", \n",
    "                  shell=True, check=True)\n",
    "    subprocess.run(\"unzip -j weights.zip\", shell=True, check=True)\n",
    "    subprocess.run(\"\"\"for i in _data-00001-of-00002 _data-00000-of-00002 _index; \n",
    "                     do out=`echo ${i} | sed \"s/_/weights./\"`; mv ${i} ${out}; \n",
    "                     done\"\"\", shell=True, check=True)\n",
    "\n",
    "# Go to DeepBleed directory\n",
    "os.chdir(str(deepbleed_dir))\n",
    "print(f\"\\nCurrent working directory: {os.getcwd()}\")\n",
    "\n",
    "# Run the prediction\n",
    "print(\"\\nRunning prediction...\")\n",
    "cmd = f\"python3 predict.py --indir {input_dir} --outdir {output_dir} --weights {weights_dir}/weights\"\n",
    "print(f\"Command: {cmd}\")\n",
    "\n",
    "result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "print(\"\\nOutput:\", result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"\\nErrors:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec 10 14:52:06 2024       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\r\n",
      "|-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  Tesla V100-SXM2-16GB           On  |   00000000:18:00.0 Off |                    0 |\r\n",
      "| N/A   42C    P0             44W /  300W |       1MiB /  16384MiB |      0%   E. Process |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "|   1  Tesla V100-SXM2-16GB           On  |   00000000:3B:00.0 Off |                    0 |\r\n",
      "| N/A   37C    P0             42W /  300W |       1MiB /  16384MiB |      0%   E. Process |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "|   2  Tesla V100-SXM2-16GB           On  |   00000000:86:00.0 Off |                    0 |\r\n",
      "| N/A   39C    P0             44W /  300W |       1MiB /  16384MiB |      0%   E. Process |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "|   3  Tesla V100-SXM2-16GB           On  |   00000000:AF:00.0 Off |                    0 |\r\n",
      "| N/A   43C    P0             46W /  300W |       1MiB /  16384MiB |      0%   E. Process |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "|  No running processes found                                                             |\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing images:   0%|          | 0/4492 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 GPUs\n",
      "Found 4492 images to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  29%|██▊       | 1289/4492 [1:53:15<4:40:48,  5.26s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import multiprocessing as mp\n",
    "from queue import Empty\n",
    "\n",
    "def configure_gpu_memory():\n",
    "    \"\"\"Configure TF 2.1.0 GPU memory growth\"\"\"\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"GPU memory configuration failed: {e}\")\n",
    "\n",
    "def process_single_image(gpu_queue, img_path, base_paths, result_queue):\n",
    "    \"\"\"Process a single image with the next available GPU\"\"\"\n",
    "    try:\n",
    "        # Get GPU ID from queue\n",
    "        gpu_id = gpu_queue.get()\n",
    "        deepbleed_dir, input_dir, output_dir, weights_dir = base_paths\n",
    "        \n",
    "        # Configure GPU for this process\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "        configure_gpu_memory()\n",
    "        \n",
    "        # Create GPU-specific input directory\n",
    "        gpu_input_dir = input_dir / f\"gpu_{gpu_id}\"\n",
    "        if gpu_input_dir.exists():\n",
    "            shutil.rmtree(str(gpu_input_dir))\n",
    "        gpu_input_dir.mkdir(parents=True)\n",
    "        \n",
    "        # Copy image to GPU-specific input directory\n",
    "        shutil.copy2(img_path, gpu_input_dir / img_path.name)\n",
    "        \n",
    "        # Change to DeepBleed directory\n",
    "        os.chdir(str(deepbleed_dir))\n",
    "        \n",
    "        # Run DeepBleed command\n",
    "        cmd = f\"python3 predict.py --verbose --indir {gpu_input_dir} --outdir {output_dir} --weights {weights_dir}/weights\"\n",
    "        \n",
    "        result = subprocess.run(cmd, \n",
    "                              shell=True, \n",
    "                              text=True,\n",
    "                              capture_output=True)\n",
    "        \n",
    "        success = result.returncode == 0\n",
    "        message = \"Success\" if success else result.stderr\n",
    "        \n",
    "    except Exception as e:\n",
    "        success = False\n",
    "        message = str(e)\n",
    "    \n",
    "    finally:\n",
    "        # Clean up\n",
    "        if 'gpu_input_dir' in locals() and gpu_input_dir.exists():\n",
    "            shutil.rmtree(str(gpu_input_dir))\n",
    "        \n",
    "        # Put GPU back in queue and send result\n",
    "        gpu_queue.put(gpu_id)\n",
    "        result_queue.put((img_path.name, success, message))\n",
    "\n",
    "def run_deepbleed():\n",
    "    # Configure GPU memory growth at start\n",
    "    configure_gpu_memory()\n",
    "    \n",
    "    # Setup paths\n",
    "    root_dir = Path('/projectnb/ec523kb/projects/hemorrhage-classification')\n",
    "    training_dir = root_dir / 'stage_2_train_sorted_nifti_pruned/trainingImages'\n",
    "    deepbleed_dir = root_dir / 'stage_2_train_sorted_nifti_pruned/deepbleed'\n",
    "    input_dir = root_dir / 'stage_2_train_sorted_nifti_pruned/deepbleed_input'\n",
    "    output_dir = root_dir / 'stage_2_train_sorted_nifti_pruned/deepbleed_output'\n",
    "    weights_dir = deepbleed_dir / 'weights'\n",
    "    \n",
    "    # Get available GPUs\n",
    "    num_gpus = len(tf.config.experimental.list_physical_devices('GPU'))\n",
    "    print(f\"Found {num_gpus} GPUs\")\n",
    "    \n",
    "    if num_gpus == 0:\n",
    "        print(\"No GPUs found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Create directories\n",
    "    input_dir.mkdir(exist_ok=True)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Get list of all images\n",
    "    training_images = list(training_dir.glob('*.nii*'))\n",
    "    print(f\"Found {len(training_images)} images to process\")\n",
    "    \n",
    "    # Create GPU queue and fill it with available GPUs\n",
    "    gpu_queue = mp.Queue()\n",
    "    for i in range(num_gpus):\n",
    "        gpu_queue.put(i)\n",
    "    \n",
    "    # Create result queue\n",
    "    result_queue = mp.Queue()\n",
    "    \n",
    "    # Prepare base paths\n",
    "    base_paths = (deepbleed_dir, input_dir, output_dir, weights_dir)\n",
    "    \n",
    "    # Process images with managed GPU allocation\n",
    "    processes = []\n",
    "    completed = 0\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    failed_images = []\n",
    "    \n",
    "    with tqdm(total=len(training_images), desc=\"Processing images\") as pbar:\n",
    "        # Start initial batch of processes\n",
    "        while len(processes) < num_gpus and completed < len(training_images):\n",
    "            p = mp.Process(\n",
    "                target=process_single_image,\n",
    "                args=(gpu_queue, training_images[completed], base_paths, result_queue)\n",
    "            )\n",
    "            p.start()\n",
    "            processes.append((p, training_images[completed].name))\n",
    "            completed += 1\n",
    "            time.sleep(5)  # Delay between process starts\n",
    "        \n",
    "        # Process remaining images and handle completions\n",
    "        while processes or completed < len(training_images):\n",
    "            # Start new processes if GPUs are available\n",
    "            while len(processes) < num_gpus and completed < len(training_images):\n",
    "                p = mp.Process(\n",
    "                    target=process_single_image,\n",
    "                    args=(gpu_queue, training_images[completed], base_paths, result_queue)\n",
    "                )\n",
    "                p.start()\n",
    "                processes.append((p, training_images[completed].name))\n",
    "                completed += 1\n",
    "                time.sleep(5)\n",
    "            \n",
    "            # Check for completed processes\n",
    "            for i in range(len(processes) - 1, -1, -1):\n",
    "                process, img_name = processes[i]\n",
    "                try:\n",
    "                    result_name, success, message = result_queue.get_nowait()\n",
    "                    if success:\n",
    "                        successful += 1\n",
    "                        print(f\"\\nSuccessfully processed: {result_name}\")\n",
    "                    else:\n",
    "                        failed += 1\n",
    "                        failed_images.append(result_name)\n",
    "                        print(f\"\\nFailed to process {result_name}: {message}\")\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "                    process.join()\n",
    "                    processes.pop(i)\n",
    "                    \n",
    "                except Empty:\n",
    "                    if not process.is_alive():\n",
    "                        failed += 1\n",
    "                        failed_images.append(img_name)\n",
    "                        processes.pop(i)\n",
    "                        pbar.update(1)\n",
    "            \n",
    "            time.sleep(1)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nProcessing Complete!\")\n",
    "    print(f\"Successfully processed: {successful}\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "    if failed_images:\n",
    "        print(\"\\nFailed images:\")\n",
    "        for img in failed_images:\n",
    "            print(f\"  - {img}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Use spawn method for clean process creation\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "    run_deepbleed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Wed Dec 11 00:29:34 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100-SXM2-16GB           On  |   00000000:18:00.0 Off |                    0 |\n",
      "| N/A   46C    P0             74W /  300W |     384MiB /  16384MiB |      0%   E. Process |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2-16GB           On  |   00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   37C    P0             43W /  300W |       4MiB /  16384MiB |      0%   E. Process |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2-16GB           On  |   00000000:86:00.0 Off |                    0 |\n",
      "| N/A   37C    P0             43W /  300W |       4MiB /  16384MiB |      0%   E. Process |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2-16GB           On  |   00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   40C    P0             43W /  300W |       4MiB /  16384MiB |      0%   E. Process |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   4047160      C   python3                                       380MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!python3 -c \"import tensorflow as tf; print(tf.__version__)\"\n",
    "!nvidia-smi  # To check CUDA version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
